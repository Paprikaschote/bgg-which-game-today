services:
    api:
        image: localai/localai:latest-aio-cpu
        # For Nvidia GPUs decomment one of the following (cuda11 or cuda12):
        # image: localai/localai:latest-aio-gpu-nvidia-cuda-11
        # image: localai/localai:latest-aio-gpu-nvidia-cuda-12
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
            interval: 1m
            timeout: 20m
            retries: 5
        ports:
            - 8080:8080
        environment:
            - DEBUG=true
        volumes:
            - ./models:/build/models:cached
            # decomment the following piece if running with Nvidia GPUs
            # deploy:
            #   resources:
            #     reservations:
            #       devices:
            #         - driver: nvidia
            #           count: 1
            #           capabilities: [gpu]
        
    qdrant:
        image: qdrant/qdrant:v1.11.0
        ports:
            - "6333:6333"
            - "6334:6334"
        volumes:
            - ./qdrant_storage:/qdrant/storage:z
